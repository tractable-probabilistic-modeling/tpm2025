---
layout: default
permalink: /program/
title: Program
---
The workshop will happen in person at the Universitat Pompeu Fabra, Barcelona, Spain on July 19th, 2024 in Room 40.008.

<table class="table table-striped">
    <colgroup>
       <col span="1" style="width: 30%;">
       <col span="1" style="width: 70%;">
    </colgroup>
    <thead>
    <tr>
        <th scope="col">Time</th>
        <th scope="col">Session</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>9:00 - 9:15</td>
        <td>Welcome and Best Paper Awards</td>
    </tr>
    <tr>
        <td>9:15 - 10:00</td>
        <td>Spotlights</td>
    </tr>
    <tr>
        <td>10:00 - 10:30</td>
        <td>Coffee Break</td>
    </tr>
    <tr>
        <td>10:30 - 11:30</td>
        <td>
        <p>Invited Talk by <strong>Devendra Singh Dhami</strong></p>
        <p><small>
        <b>Title:</b> Causality and Probabilistic Circuits: So Close Yet So Far <br />
        <b>Abstract:</b> The intricate and expansive nature of our world often demands large models with numerous variables. However, crafting a model that strikes the balance between accuracy and timely predictions can present significant challenges. I will talk about how the construct of probabilistic circuits (PCs) can be used to alleviate both challenges and will show how PCs can be used to answer both interventional and counterfactual queries.  I further show how PCs can help in answering causal questions in real-world benchmarks thereby taking a step towards solving the scalability issues in causal machine learning.
        </small></p>
        </td>
    </tr>
    <tr>
        <td>11:30 - 12:30</td>
        <td>
        <p>Invited Talk by <strong>Yingzhen Li</strong></p>
        <p><small>
        <b>Title:</b> On Advancing Approximate Inference in the Era of Big AI <br />
        <b>Abstract:</b> A paradigm shift is on-going in the deep learning field, where researchers are racing in training multimodal foundation models bigger than ever. Currently non-probabilistic approaches dominate the playground of big foundation models, where most probabilistic/Bayesian approaches are prohibitively expensive to compete. But the practitioner's impression that “probabilistic models are always more expensive than simple deterministic neural networks” is not true, and I will discuss an example from my previous work which has trained Bayesian neural networks with memory complexity lower than deterministic neural networks, and show recent progress on reducing the time complexity as well. I believe that, apart from making great progress on methods with better approximation accuracies and guarantees, to better engage with next-gen deep learning research, computational complexity reduction should be back to the main stage again for future approximate inference research.
        </small></p>
        </td>
    </tr>
    <tr>
        <td>12:30 - 14:00</td>
        <td>Lunch Break</td>
    </tr>
    <tr>
        <td>14:00 - 15:00</td>
        <td>Award Talks</td>
    </tr>
    <tr>
        <td>15:00 - 16:00</td>
        <td>
        <p>Invited Talk by <strong>Feras Saad</strong></p>
        <p><small>
        <b>Title:</b> Scalable Spatiotemporal Prediction with Bayesian Neural Fields <br />
        <b>Abstract:</b> Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and tractable enough to handle large prediction problems. This talk discusses the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain. The model can be used for data-analysis tasks including forecasting, interpolation, and variography. <br />

BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. BayesNF delivers improvements over prominent statistical and machine-learning baselines on diverse prediction problems from climate and public health datasets containing tens to hundreds of thousands of measurements. BayesNF is available as a user-friendly <a href="https://github.com/google/bayesnf">open-source software package</a> that operates on modern GPU and TPU accelerators via the JAX machine learning platform.
        </small></p>
        </td>
    </tr>
    <tr>
        <td>16:00 - 17:30</td>
        <td>Poster Session and Coffee Break</td>
    </tr>
    </tbody>
</table>
